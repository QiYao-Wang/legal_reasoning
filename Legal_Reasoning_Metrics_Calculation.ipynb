{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4230076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Add the desired path to sys.path\n",
    "sys.path.append('') # Append openai library path to sys.path\n",
    "# print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d578fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9195794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "import openai\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(dotenv_path='') # Add the path to the 'env' containing the relevant OPENAI API key here, preferably to use GPT-4o\n",
    "# Now you can access the environment variable\n",
    "api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16e1d7a",
   "metadata": {},
   "source": [
    "### Loading the 'Civ. Pro.' Dataset\n",
    "The dataset should contain the columns for: 1. Legal Context('Context' here in a dataframe 'df') 2. Question ('Question') 3. Options ('Options'), 4. Expert Answers ('Analysis') and the LLM generated responses to be evaluated for errors (Which in our case, are named as 'Response(LLM X)' for any LLM X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e180409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "input_file = r\"\" # Add the csv containing the 'Civ. Pro' Dataset\n",
    "df = pd.read_csv(input_file)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28d5fd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "# OpenAI API call handling for timout issues\n",
    "def get_text_completion_from_GPT(system_prompt, prompt, model=\"gpt-4o\", max_retries=3, retry_delay=2):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": prompt}]\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                temperature=0,  # Degree of randomness of the model's output\n",
    "                frequency_penalty=0,\n",
    "                presence_penalty=0,\n",
    "                seed = 42\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except (requests.exceptions.Timeout, requests.exceptions.RequestException) as e:\n",
    "            # print(f\"Attempt {attempt + 1} failed. Retrying after {retry_delay} seconds.\")\n",
    "            time.sleep(retry_delay)\n",
    "    # If all attempts fail, raise the last exception\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4677617e",
   "metadata": {},
   "source": [
    "# Metric Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d04598",
   "metadata": {},
   "source": [
    "## Step-Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590bb0fe",
   "metadata": {},
   "source": [
    "#### Counting the steps in reasoning chains for LLM 'X'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e5bd89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_counter(llm_rc):\n",
    "    step_counter_system_prompt = \"\"\"You are an expert in counting the number of steps in a reasoning chain generated as an explanation to legal MCQ. You are provided with a reasoning chain which consists of premises and a conclusion.\n",
    "Your task is to count the number of steps by counting all the premises and the conclusion.     \n",
    "\n",
    "### Instructions:\n",
    "1. Number of Steps = Number of Premises + Conclusion.\n",
    "2. If a premise contains analysis of individual options, then count each of those anlayses as single options.\n",
    "3. The final conclusion along with the subsequent choosing of an option is considered as a single step.\n",
    "\n",
    "Provide the output in the following key-value format:\n",
    "num_of_premises:\n",
    "num_of_steps:\n",
    "\n",
    "Example 1 of counting number of steps in a reasoning chain:\n",
    "Reasoning Chain:\n",
    "[1] The question asks about Marla's domicile based on her current residence in Denver, Colorado, and her intentions regarding her stay there.\n",
    "[2] According to the legal context, a person's domicile is determined by the state where they reside with the intent to remain indefinitely.\n",
    "[3] Marla moved to Denver, Colorado, to attend a two-year hair stylist program. She was unsure about her career choice and had plans to leave the program if she didn't like it. She also mentioned that she might look for work in Denver or other western states, including Montana.\n",
    "[4] Marla's lease in Denver is for six months, which indicates that she does not have an open-ended intention to remain in Colorado indefinitely.\n",
    "[5] The fact that Marla was domiciled in Montana before moving to Denver does not automatically make her domiciled in Montana again. Domicile is determined by the present intent to remain indefinitely in a state, not by past domicile.\n",
    "[6] Based on the information provided, Marla does not meet the requirement of residing in Colorado with the intent to remain indefinitely.\n",
    "[Final Answer] A. remains domiciled in Montana.\n",
    "\n",
    "YOUR RESPONSE:\n",
    "num_of_premises: 5 \n",
    "num_of_steps: 6\n",
    "\n",
    "Example 2 of counting number of steps in a reasoning chain:\n",
    "1. **Understanding Rule 26(a)(1) Requirements**: Rule 26(a)(1) mandates the disclosure of information that a party may use to support its claims or defenses. This includes documents and other evidentiary material that are relevant to the case.\n",
    "2. **Relevance of the Letter**: The letter from an individual claiming copyright over the story is directly relevant to Fremont Publishing Company's claim of copyright in the lawsuit against New Era Press. The letter challenges Fremont's assertion of copyright ownership, which is central to the case.\n",
    "3. **Obligation to Disclose**: Under Rule 26(a)(1), Fremont should have disclosed the letter as it pertains to the computation of damages and the establishment of a claim (copyright ownership in this case). The letter is a piece of evidence that Fremont might use to defend against the claim of copyright if it impacts the validity of their ownership claim.\n",
    "4. **Rule 26(e)(1) - Duty to Supplement**: This rule requires a party to supplement or correct its disclosure upon learning that the initial disclosure was either incomplete or incorrect. Fremont's failure to initially disclose the letter necessitates a need for supplemental disclosure.\n",
    "5. **Analysis of Options**:\n",
    "   - **Option A (Sanctions under Rule 37(c)(1))**: This option is typically considered when a party fails to disclose information that should have been disclosed under Rule 26(a). However, sanctions are generally applied for more egregious or harmful nondisclosures.\n",
    "   - **Option B (Supplemental Disclosure)**: This is the most fitting response as it directly addresses the need for Fremont to correct its oversight or intentional nondisclosure by providing the letter.\n",
    "   - **Option C (Rule 34 Request for Production)**: While this could theoretically obtain the letter, it is not the most direct or appropriate method given that the issue here is Fremont's failure to comply with mandatory disclosure rules.\n",
    "   - **Option D (Cannot obtain the letter)**: This is incorrect because the letter is clearly relevant and should have been disclosed under Rule 26(a).\n",
    "\n",
    "[Final Answer: Option B should move to require supplemental disclosure of the letter, under Rules 26(a)(1) and 26(e)(1). This option directly addresses the failure to disclose relevant information and seeks to rectify this by enforcing the rules designed to ensure fairness and transparency in the discovery process.]\n",
    "\n",
    "YOUR RESPONSE:\n",
    "num_of_premises: 8 <Since each option analysis is counted as one step>\n",
    "num_of_steps: 9\n",
    "\"\"\"\n",
    "    input_prompt = f\"\"\"{llm_rc}\"\"\"\n",
    "    extracted_text = get_text_completion_from_GPT(step_counter_system_prompt, input_prompt)\n",
    "    return extracted_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeb70de",
   "metadata": {},
   "source": [
    "### Loading and saving data in an output file 'LLM X'\n",
    "The path to an output file named 'LLM X' (where LLM X is to be substituted with the actual name of a LLM) is to be provided where the computed results are to be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efbbac47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "output_path = r'' # Output Path where to store the Metrics computation results done in this notebook for LLM X\n",
    "llm_x_output_file = output_path\n",
    "if not os.path.exists(llm_x_output_file):\n",
    "    # Create an empty file if it doesn't exist\n",
    "    pd.DataFrame().to_csv(llm_x_output_file)\n",
    "\n",
    "llm_x_output_df = pd.read_csv(llm_x_output_file)\n",
    "for idx, row in df.iterrows():\n",
    "  # Get LLM X's Reasoning Chain\n",
    "    llm_rc = row['Response(LLM X)'] # Please subsititute 'LLM X' inside 'Response(LLM X)' with the appropriate LLM name\n",
    "    print(f\"FOR SAMPLE NUMBER: {idx+1}\")\n",
    "    print(f\"The LLM Reasoning Chain is:\\n {llm_rc}\\n\")\n",
    "    # Calculate the number of steps in each reasoning chain\n",
    "    counter_output = step_counter(llm_rc)\n",
    "    print(counter_output)\n",
    "    print(\"=\"*124)\n",
    "    # Using regular expressions to extract the numbers\n",
    "    num_of_steps = int(re.search(r'num_of_steps:\\s*(\\d+)', counter_output).group(1))\n",
    "    llm_x_output_df.at[idx,'Counter Output'] = counter_output\n",
    "    llm_x_output_df.at[idx,'Number of Steps'] = num_of_steps\n",
    "    llm_x_output_df.to_csv(llm_x_output_file, index=False) \n",
    "    \n",
    "# llm_x_output_df and llm_x_output_file will be used in the subsequent cells as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e2c4fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split 'Premise and Conclusion Errors' column into 'Premise Errors' and 'Conclusion Errors'\n",
    "def extract_errors_split(text):\n",
    "    # Splitting based on the keywords for premise errors\n",
    "    premise_split = text.split('ERROR AGGREGATION AT PREMISE LEVEL:')\n",
    "    if len(premise_split) > 1:\n",
    "        premise_level_errors = premise_split[1].split('ERROR AT CONCLUSION LEVEL:')[0].strip()\n",
    "    else:\n",
    "        premise_level_errors = None\n",
    "\n",
    "    # Splitting based on the keywords for conclusion errors\n",
    "    conclusion_split = text.split('ERROR AT CONCLUSION LEVEL:')\n",
    "    if len(conclusion_split) > 1:\n",
    "        conclusion_level_errors = conclusion_split[1].strip()\n",
    "    else:\n",
    "        conclusion_level_errors = None\n",
    "\n",
    "    return premise_level_errors, conclusion_level_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d022a4e",
   "metadata": {},
   "source": [
    "### Loading data from auto-eval output file for 'LLM X'\n",
    "Load the file for LLM X which contains the column: 'Premise and Conclusion Errors' as processed in 'Legal-Reasoning-Auto-Evaluator-Pipeline' notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9a8132fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the file for LLM X which contains the column: 'Premise and Conclusion Errors' as processed in 'Legal-Reasoning-Auto-Evaluator-Pipeline' notebook\n",
    "import pandas as pd\n",
    "llm_x_input_file = r\"\" # Add the file which contains the above column for any LLM X\n",
    "llm_x_input_df = pd.read_csv(llm_x_input_file)\n",
    "llm_x_output_df[['Premise Errors', 'Conclusion Errors']] = llm_x_input_df['Premise and Conclusion Errors'].apply(lambda x: pd.Series(extract_errors_split(x)))\n",
    "llm_x_output_df.to_csv(llm_x_output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f2792d",
   "metadata": {},
   "source": [
    "## Premise-Error Label-Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38a4f09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_extractor(prem_errors):\n",
    "    label_extraction_system_prompt = \"\"\"You are an expert in labelling the errors in the summary of premises and analyses presented to you. You have label them as either 'MISINTERPRETATION', 'FACTUAL HALLUCINATION' OR 'IRRELEVANT PREMISE'. If a premise (and its error explanation) contain mentions of multiple errors then all the appropriate labels to the premise.     \n",
    "\n",
    "### Instructions:\n",
    "1. A premise should .\n",
    "2. If a premise contains analysis of individual options, then count each of those analyses as single steps.\n",
    "3. For a permise, mention a label only ONCE, i.e. for example, do NOT label 'Premise 1: MISINTERPRETATION, MISINTERPRETATION'\n",
    "4. If a point starts with the heading of 'Conclusion', do NOT label it.\n",
    "\n",
    "\n",
    "Example 1 of labelling errors:\n",
    "Provided Summary of Premise-Level Errors:\n",
    "\n",
    "1. **Premise 4: Marla's Intent**\n",
    "   - **Error**: Misinterprets Marla's intent.\n",
    "   - **Explanation**: The premise incorrectly suggests ambiguity in Marla's intent. According to the legal context, Marla's plans are open-ended, meaning she does not have definite plans to leave Denver. The ambiguity mentioned does not align with the legal context's explanation of \"indefinite intent.\"\n",
    "\n",
    "YOUR RESPONSE:\n",
    "Premise 4: MISNTERPRETATION\n",
    "\n",
    "Example 2 of labelling errors:\n",
    "Provided Summary of Premise-Level Errors:\n",
    "\n",
    "1. **Premise 3:**\n",
    "   - **Error:** Misinterpretation of the legal context.\n",
    "   - **Description:** Incorrectly states that a case can be brought in federal court if the total amount in controversy exceeds the jurisdictional limit when suing multiple defendants. The legal context specifies that the plaintiff must seek more than $75,000 from each defendant individually, not in total.\n",
    "\n",
    "2. **Premise 6:**\n",
    "   - **Error:** Misinterpretation of the amount-in-controversy requirement.\n",
    "   - **Description:** Incorrectly asserts that the case can be brought in federal court if the total amount in controversy exceeds the jurisdictional limit by aggregating claims from multiple plaintiffs. Each plaintiff must individually meet the $75,000 threshold.\n",
    "\n",
    "3. **Premise 7:**\n",
    "   - **Error:** Misinterpretation based on the previous error.\n",
    "   - **Description:** The case cannot be brought in federal court because neither Larry nor Moe individually meets the $75,000 requirement.\n",
    "\n",
    "4. **Premise 9:**\n",
    "   - **Error:** Misinterpretation of the amount-in-controversy requirement.\n",
    "   - **Description:** Incorrectly includes the counterclaim in the amount-in-controversy calculation. The amount-in-controversy requirement must be assessed based only on the plaintiffâ€™s claim, without regard to the value of any counterclaim.\n",
    "\n",
    "5. **Premise 10:**\n",
    "   - **Error:** Misinterpretation based on the previous error.\n",
    "   - **Description:** The case cannot be brought in federal court because Larry's claim alone does not meet the $75,000 requirement.\n",
    "\n",
    "6. **Premise 15:**\n",
    "   - **Error:** Misinterpretation of the amount-in-controversy requirement.\n",
    "   - **Description:** Incorrectly aggregates the claims against Curly and Dr. Moe. Each defendant must individually meet the $75,000 threshold.\n",
    "\n",
    "7. **Premise 16:**\n",
    "   - **Error:** Misinterpretation based on the previous error.\n",
    "   - **Description:** The case cannot be brought in federal court because Dr. Moe's individual liability does not meet the $75,000 requirement.\n",
    "\n",
    "8. **Option A Analysis:**\n",
    "   - **Error:** Misinterpretation of the amount-in-controversy requirement.\n",
    "   - **Description:** Incorrectly aggregates the claims of Larry and Moe. Each plaintiff's claim must individually meet the jurisdictional amount.\n",
    "\n",
    "9. **Option B Analysis:**\n",
    "   - **Error:** Misinterpretation of the amount-in-controversy requirement.\n",
    "   - **Description:** Incorrectly includes the counterclaim in the amount-in-controversy calculation. The jurisdictional amount must be met by the plaintiff's claim alone.\n",
    "\n",
    "10. **Option D Analysis:**\n",
    "    - **Error:** Misinterpretation of the amount-in-controversy requirement.\n",
    "    - **Description:** Incorrectly aggregates the claims against Curly and Dr. Moe. Each claim must individually meet the jurisdictional amount.\n",
    "\n",
    "YOUR RESPONSE:\n",
    "Premise 3: MISNTERPRETATION\n",
    "Premise 6: MISNTERPRETATION\n",
    "Premise 7: MISNTERPRETATION\n",
    "Premise 9: MISNTERPRETATION\n",
    "Premise 10: MISNTERPRETATION\n",
    "Premise 15: MISNTERPRETATION\n",
    "Premise 16: MISNTERPRETATION\n",
    "Option A Analysis: MISNTERPRETATION\n",
    "Option B Analysis: MISNTERPRETATION\n",
    "Option D Analysis: MISNTERPRETATION\n",
    "\n",
    "Example 3 of labelling errors:\n",
    "Provided Summary of Premise-Level Errors:\n",
    "1. **Premise 2:**\n",
    "   - **Error Category:** Factual Inconsistency Hallucination and Misinterpretation\n",
    "   - **Explanation:** Incorrectly states that all plaintiffs are from Virginia, whereas Gerry is actually from Massachusetts. This misinterpretation leads to an incorrect conclusion about diversity jurisdiction and misinterprets the Strawbridge Rule by suggesting diversity jurisdiction is proper despite the presence of a plaintiff (Gerry) from a different state (Massachusetts).\n",
    "\n",
    "2. **Premise 3:**\n",
    "   - **Error Category:** Misinterpretation and Irrelevant Premise\n",
    "   - **Explanation:** Incorrectly states that Madison and Lafayette are from the same state, which is not true. Madison is from Virginia and Lafayette is from Maryland. Additionally, it misinterprets the legal context regarding corporate citizenship by incorrectly concluding that diversity jurisdiction is not proper. Washington Corporation is a citizen of Maryland, and Madison is from Virginia, making the premise irrelevant and incorrect.\n",
    "\n",
    "3. **Premise 4:**\n",
    "   - **Error Category:** Misinterpretation and Irrelevant Premise\n",
    "   - **Explanation:** Incorrectly concludes that diversity jurisdiction is proper by failing to recognize that Adams Corporation is a citizen of both Delaware and Virginia. Since Madison is from Virginia and Adams Corporation has its principal place of business in Virginia, there is no diversity jurisdiction. This misinterpretation makes the premise irrelevant and incorrect.\n",
    "\n",
    "YOUR RESPONSE:\n",
    "Premise 2: FACTUAL HALLUCINATION, MISINTERPRETATION\n",
    "Premise 3: MISNTERPRETATION, IRRELEVANT PREMISE\n",
    "Premise 4: MISNTERPRETATION, IRRELEVANT PREMISE\n",
    "\n",
    "Example 4 of labelling errors:\n",
    "Provided Summary of Premise-Level Errors:\n",
    "1. **Premise 1:**\n",
    "   - **Error Category:** Irrelevant/Misinterpretation\n",
    "   - **Explanation:** The premise incorrectly discusses Pennoyer v. Neff and the \"presence theory,\" which is outdated and irrelevant to the modern context of minimum contacts jurisdiction. The focus should be on the concepts of foreseeability and purposeful availment.\n",
    "\n",
    "2. **Premise 2:**\n",
    "   - **Error Category:** Irrelevant\n",
    "   - **Explanation:** The background information does not mention International Shoe v. Washington or the \"minimum contacts\" theory, nor does it explain the expansion of jurisdictional basis through this theory.\n",
    "\n",
    "3. **Premise 9:**\n",
    "   - **Error Category:** Misinterpretation\n",
    "   - **Explanation:** This premise misinterprets the legal context by incorrectly concluding that physical presence in the state is necessary for establishing personal jurisdiction. Boyarin's deliberate contact with Mercy Hospital in Virginia to interfere with a contract is sufficient to establish purposeful availment, making Option B the correct answer.\n",
    "\n",
    "4. **Conclusion:**\n",
    "   - **Error Category:** Misinterpretation\n",
    "   - **Explanation:** The conclusion is based on the misinterpretation in Premise 9, which incorrectly applies the legal principles of personal jurisdiction.\n",
    "\n",
    "YOUR RESPONSE:\n",
    "Premise 1: IRRELEVANT PREMISE, MISINTERPRETATION\n",
    "Premise 2: IRRELEVANT PREMISE\n",
    "Premise 9: MISINTERPRETATION\n",
    "\"\"\"\n",
    "    input_prompt = f\"\"\"{prem_errors}\"\"\"\n",
    "    extracted_text = get_text_completion_from_GPT(label_extraction_system_prompt, input_prompt)\n",
    "    return extracted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c9428a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in llm_x_output_df.iterrows():\n",
    "    # Get LLM X's Premise-level errors\n",
    "    prem_err = row['Premise Errors']\n",
    "    print(f\"FOR SAMPLE NUMBER: {idx+1}\")\n",
    "    print(f\"The Premise Errors Summary is:\\n {prem_err}\\n\")\n",
    "    # Calculate the number of steps in each reasoning chain\n",
    "    label_output = label_extractor(prem_err)\n",
    "    print(label_output)\n",
    "    print(\"=\"*124)\n",
    "    llm_x_output_df.at[idx,'Premise Error Labels'] = label_output\n",
    "    llm_x_output_df.to_csv(llm_x_output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83fa8df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute number of premise-level errors and conclusion-level errors for Soundness and Correctness Metric Calculation\n",
    "llm_x_output_df['Number of Premise Errors'] = llm_x_output_df['Premise Error Labels'].apply(lambda x: str(x).count('Premise '))\n",
    "llm_x_output_df['Soundness Score'] = (1 - llm_x_output_df['Number of Premise Errors'] / (llm_x_output_df['Number of Steps'] - 1))\n",
    "llm_x_output_df['Correctness Score'] = llm_x_output_df['Conclusion Errors'].str.contains(r'\\bCORRECT CONCLUSION\\b(?!\\s+FROM)', regex=True).astype(int)\n",
    "llm_x_output_df['Correct Conclusion Score'] = llm_x_output_df['Conclusion Errors'].str.contains(r'CORRECT CONCLUSION').astype(int) # Represents the Accuracy Metric score\n",
    "llm_x_output_df.to_csv(llm_x_output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01255c9d",
   "metadata": {},
   "source": [
    "## Conclusion-Error Label-Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2240dd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for conclusion error frequencies\n",
    "llm_x_output_df['Count(CORRECT CONCLUSION)'] = llm_x_output_df['Conclusion Errors'].str.contains(r'\\bCORRECT CONCLUSION\\b(?!\\s+FROM)', regex=True).astype(int)\n",
    "llm_x_output_df['Count(CORRECT CONCLUSION FROM FALSE PREMISES'] = llm_x_output_df['Conclusion Errors'].str.contains(r'CORRECT CONCLUSION FROM FALSE PREMISES').astype(int)\n",
    "llm_x_output_df['Count(HALLUCINATION)'] = llm_x_output_df['Conclusion Errors'].str.contains(r'HALLUCINATION').astype(int) # Represents the 'Correct Conclusion with Hallucinated Content' error category\n",
    "llm_x_output_df['Count(WRONG CONCLUSION FROM FALSE PREMISES)'] = llm_x_output_df['Conclusion Errors'].str.contains(r'WRONG CONCLUSION FROM FALSE PREMISES').astype(int)\n",
    "llm_x_output_df['Count(WRONG CONCLUSION FROM INCOMPLETE PREMISES)'] = llm_x_output_df['Conclusion Errors'].str.contains(r'WRONG CONCLUSION FROM INCOMPLETE PREMISES').astype(int)\n",
    "llm_x_output_df.to_csv(llm_x_output_file, index=False)\n",
    "# View the DataFrame to ensure the counts are correct\n",
    "llm_x_output_file.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
